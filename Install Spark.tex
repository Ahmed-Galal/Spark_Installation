\documentclass[]{article}

%opening
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage{xcolor}
\usepackage{graphicx}


\begin{document}
\begin{titlepage}
	\centering

	{\LARGE Egyptian Big Data Geeks 3rd Event \par}
	\vspace{1cm}
	{\Large Installing Spark and Scala\par}
	\vspace{1.5cm}
	{\huge\bfseries Mostafa Alaa Mohamed\par}
	{\Large\itshape mustafaalaa.mohamed@gmail.com\par}

	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}

\begin{abstract}
This Document introduce How to install spark into local Machine. This document doesn't have any copy-writes and I collected and copied from Internet content to be easy for who need to install spark into one document. This document generated by \LaTeX  \ and code will be found into the following link
\end{abstract}

\section{Installing Apache Spark and Scala}

\subsection{MacOS}
\begin{itemize}

	\item Install Apache Spark using Homebrew.
		\begin{itemize}
			\item Install Homebrew if you don’t have it already by entering this from a terminal prompt: /usr/bin/ruby -e "\$(curl \-fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
			\item Enter brew install apache-spark
			\item Create a log4j.properties file via
			cd /usr/local/Cellar/apache-spark/2.0.0/libexec/conf
			cp log4j.properties.template log4j.properties
			(substituted 2.0.0 for the version actually installed)
			\item Edit the log4j.properties file and change the log level from INFO to ERROR on
			log4j.rootCategory.
		\end{itemize}

	\item Install the Scala IDE from http://scala-ide.org/download/sdk.html
		\item Test it out!
			\begin{itemize}
					\item ./bin/spark-shell

					\item val textFile = sc.textFile("README.md") // textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:25


					\item textFile.count() // Number of items in this RDD
					\item res0: Long = 126

					\item You should show a count of the number of lines in that file.
					\item Hit control-D to exit the spark shell, and close the console window
			\end{itemize}

\end{itemize}


\subsection{Linux}

\textbf{Note That:} The below code only tested under Fedora

\begin{itemize}
	\item Installing Scala
		\begin{itemize}
			\item curl -O http://downloads.typesafe.com/scala/2.11.7/scala-2.11.7.rpm	
			\item rpm -ivh scala-2.11.7.rpm
			\item curl -O http://ftp.unicamp.br/pub/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
			\item tar -xvzf apache-maven-3.3.9-bin.tar.gz 
			\item cd apache-maven-3.3.9
			\item rm /usr/bin/mvn 2> /dev/null
			\item alternatives --install /usr/bin/mvn mvn on \$HOME/apache-maven-3.3.9/bin/mvn 200000
			\item sudo alternatives --config mvn
		\end{itemize}
	\item Downloading and compiling Spark.
		\begin{itemize}
			\item curl -O http://mirror.nbtelecom.com.br/apache/spark/spark-2.0.1/spark-2.0.1.tgz
			\item cd spark-2.0.1/
			\item Note That: Starting Spark installation. Be very, very, very patient.
			\item build /mvn -DskipTests clean package
			\end{itemize}	
		\item Run ./spark-shell
		\item Test it out!
			\begin{itemize}
					\item ./bin/spark-shell

					\item val textFile = sc.textFile("README.md") // textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:25


					\item textFile.count() // Number of items in this RDD
					\item res0: Long = 126

					\item You should show a count of the number of lines in that file.
					\item Hit control-D to exit the spark shell, and close the console window
			\end{itemize}
		
	
\end{itemize}

\subsection{Windows}
\begin{itemize}
	
	\item Install a JDK (Java Development Kit) from http://www.oracle.com/technetwork/java/javase/downloads/index.html . Keep track of where you installed the JDK; you’ll need that later.
	\item Download a pre-built version of Apache Spark from https://spark.apache.org/downloads.html
	\item If necessary, download and install WinRAR so you can extract the .tgz file you downloaded. http://www.rarlab.com/download.htm
	\item Extract the Spark archive, and copy its contents into C:\textbackslash spark after creating that directory. You should end up with directories like c:\textbackslash spark bin, c: \textbackslash\textbackslash spark\textbackslash conf, etc.
	\item Download winutils.exe from https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe and move it into a C:\textbackslash winutils\textbackslash bin folder that you’ve created. (note, this is a 64-bit application. If you are on a 32-bit version of Windows, you will need to search for a 32-bit build of winutils.exe for Hadoop.)
	\item Open the the c:\textbackslash spark\textbackslash conf folder, ahttps://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exend make sure “File Name Extensions” is checked in the “view” tab of Windows Explorer. Rename the log4j.properties.template file to log4j.properties. Edit this file (using Wordpad or something similar) and change the error level from INFO to ERROR for log4j.rootCategory
	\item Right-click your Windows menu, select Control Panel, System and Security, and then System. Click on “Advanced System Settings” and then the “Environment Variables” button.
	\textbackslash
	\item Add the following new USER variables:
			\begin{itemize}
				\item SPARK\_HOME c:\textbackslash spark
				\item JAVA\_HOME (the path you installed the JDK to in step 1, for example C:\textbackslash Program Files\textbackslash Java\textbackslash jdk1.8.0\_101)
				\item HADOOP HOME c:\textbackslash winutils
			\end{itemize}
	\item Add the following paths to your PATH user variable:
				\begin{itemize}
					\item  \%SPARK\_HOME\%\textbackslash bin.
					\item  \%JAVA\_HOME\%\textbackslash bin.
				\end{itemize}
		\item Close the environment variable screen and the control panels.
		\item Install the Scala IDE from http://scala-ide.org/download/sdk.html
		\item Test it out!
			\begin{itemize}
					\item ./bin/spark-shell

					\item val textFile = sc.textFile("README.md") // textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:25
					\item textFile.count() // Number of items in this RDD
					\item res0: Long = 126

					\item You should show a count of the number of lines in that file.
					\item Hit control-D to exit the spark shell, and close the console window
			\end{itemize}
		
	
\end{itemize}

\section{Spark on Scala IDE}
\begin{itemize}

	\item Download the Scala IDE from http://scala-ide.org/download/sdk.html
	\item Open eclipse 
	\item choose the working directory Ex: /run/media/moustafaalaa/Main\_Hard/Work/SparkWS/
	\item File -> new -> scala project.
	\item projectname : scalademo -> next -> finish.
	\item Here you have two choices 	
\end{itemize}

	\begin{enumerate}
	\item First One import spark jars into the project direct as below 
	\begin{itemize}
	\item Right click on the project "scalademo" -> buildpath -> configure build path -> libraries ->  Add External Jars \==> choose your spark home directory -> jars -> select All jars  -> Ok -> Ok
	\item Right click on the project -> new -> scala object -> choose a name such as "scalatest" then ok.
	\item copy the below code into the object
	



\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinestyle{myScalastyle}{
  frame=tb,
  language=scala,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
}
\begin{lstlisting}[style=myScalastyle]
	
import scala.math.random
import org.apache.spark._

object scalatest {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Spark Pi")
      .setMaster("local")

    val spark = new SparkContext(conf)
    val slices = if (args.length > 0) args(0).toInt else 2
    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
    val count = spark.parallelize(1 until n, slices).map { i =>
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (x * x + y * y < 1) 1 else 0
    }.reduce(_ + _)
    println("Pi is roughly " + 4.0 * count / n)
    spark.stop()

  }
}	
\end{lstlisting}
	
	\end{itemize}
\item Second one "recommended" using maven 
Will update it soon.
	\end{enumerate}
	

\end{document}
